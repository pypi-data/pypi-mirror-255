########################################
###          DEFAULT SETTINGS        ###
########################################

# Any of those settings can be changed
# by the user. To overwrite a setting, 
# create a settings.toml or load the 
# settings from .env file or vars.
# As an example, to disable the 
# myllm object:
# settings.toml
# [default]
# myllm_enabled = false

[default]
# Dynaconf settings verification
VALUE = "On default"

# Module Enable/Disable
myllm_enabled = true

[default.myllm.template]
enabled = false
llm_model= "gpt-4"
llm_provider = "g4f.Provider.Liaobots" # Refer to https://github.com/xtekky/gpt4free
llm_provider_key = ""
max_memory = 100 # Conversation history size
timeout = 5 # time lag to wait ai response
temperature = 0
token_limit = 400
llm_prefix = "" # prefix use to filter the AI response
llm_template = """
You are a friendly AI helping me with trade monitoring. 
Be courteuous, simple and direct omitting any form of greeting or salutation.
"""

# [default.myllm.bing]
# enabled = false
# llm_model= "gpt_4"
# llm_provider = "g4f.Provider.Bing" # Refer to https://github.com/xtekky/gpt4free
# llm_provider_key = ""
# max_memory = 100 # Conversation history size
# timeout = 5 # time lag to wait ai response
# temperature = 0
# token_limit = 400
# llm_prefix = "" # prefix use to filter the AI response
# llm_template = """
# You are a friendly AI, helping me with 
# general tasks. Be courteuous, simple and direct.
# """

# [default.myllm.openai] 
# enabled = true
# llm_model= "gpt-3.5-turbo" 
# llm_provider = "" 
# llm_provider_key = "DEADBE4F"
# max_memory = 100 
# timeout = 5 
# temperature = 0
# token_limit = 1024
# llm_prefix = ""
# llm_template = """
# You are a friendly AI, helping me with 
# general tasks. Be courteuous, simple and direct.
# """

# [default.myllm.bard]
# enabled = false
# llm_model= "" # gpt-3.5-turbo" 
# llm_provider = ""
# llm_provider_key = { __Secure-1PAPISID = "", __Secure-1PSID = "", __Secure-1PSIDCC = "", __Secure-1PSIDTS = "" }
# max_memory = 100 # Conversation history size
# timeout = 5 # time lag to wait ai response
# temperature = 0
# token_limit = 1024
# llm_prefix = "" # prefix use to filter the AI response
# llm_template = """
# You are a friendly AI, helping me with 
# general tasks. Be courteuous, simple and direct.
# """

########################################
###     END OF DEFAULT SETTINGS      ###
########################################
