Metadata-Version: 2.1
Name: intel-extension-for-pytorch
Version: 2.2.0
Summary: Intel춽 Extension for PyTorch*
Home-page: https://github.com/intel/intel-extension-for-pytorch
Author: Intel Corp.
License: https://www.apache.org/licenses/LICENSE-2.0
Classifier: License :: OSI Approved :: Apache Software License
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: psutil
Requires-Dist: numpy
Requires-Dist: packaging

<div align="center">
  
Intel춽 Extension for PyTorch\*
==============================

</div>

**CPU** [游눹main branch](https://github.com/intel/intel-extension-for-pytorch/tree/main)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[游꺔Quick Start](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/getting_started.html)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[游닀Documentations](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[游끢Installation](https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=cpu&version=v2.2.0%2Bcpu)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[游눹LLM Example](https://github.com/intel/intel-extension-for-pytorch/tree/main/examples/cpu/inference/python/llm) <br>
**GPU** [游눹main branch](https://github.com/intel/intel-extension-for-pytorch/tree/xpu-main)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[游꺔Quick Start](https://intel.github.io/intel-extension-for-pytorch/xpu/latest/tutorials/getting_started.html)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[游닀Documentations](https://intel.github.io/intel-extension-for-pytorch/xpu/latest/)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[游끢Installation](https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=gpu&version=v2.1.10%2Bxpu)&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;[游눹LLM Example](https://github.com/intel/intel-extension-for-pytorch/tree/xpu-main/examples/gpu/inference/python/llm)<br>  


Intel춽 Extension for PyTorch\* extends PyTorch\* with up-to-date features optimizations for an extra performance boost on Intel hardware. Optimizations take advantage of Intel춽 Advanced Vector Extensions 512 (Intel춽 AVX-512) Vector Neural Network Instructions (VNNI) and Intel춽 Advanced Matrix Extensions (Intel춽 AMX) on Intel CPUs as well as Intel X<sup>e</sup> Matrix Extensions (XMX) AI engines on Intel discrete GPUs. Moreover, Intel춽 Extension for PyTorch\* provides easy GPU acceleration for Intel discrete GPUs through the PyTorch\* xpu device.

Intel춽 Extension for PyTorch\* provides optimizations both for eager and graph modes. However,  compared to the eager mode, the graph mode in PyTorch\* normally yields better performance from the optimization techniques like operation fusion. Intel춽 Extension for PyTorch\* amplifies them with more comprehensive graph optimizations. Both PyTorch `Torchscript` and `TorchDynamo` graph modes are supported. With `Torchscript`, we recommend using `torch.jit.trace()` as your preferred option, as it generally supports a wider range of workloads compared to `torch.jit.script()`.

## ipex.llm - Large Language Models (LLMs) Optimization

In the current technological landscape, Generative AI (GenAI) workloads and models have gained widespread attention and popularity. Large Language Models (LLMs) have emerged as the dominant models driving these GenAI applications. Starting from 2.1.0, specific optimizations for certain LLM models are introduced in the Intel춽 Extension for PyTorch\*. Check [**LLM optimizations**](./examples/cpu/inference/python/llm) for details.

### Optimized Model List

| MODEL FAMILY | MODEL NAME (Huggingface hub) | FP32 | BF16 | Static quantization INT8 | Weight only quantization INT8 | Weight only quantization INT4 |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|LLAMA| meta-llama/Llama-2-7b-hf | 游릴 | 游릴 | 游릴 | 游릴 | 游릳 | 
|LLAMA| meta-llama/Llama-2-13b-hf | 游릴 | 游릴 | 游릴 | 游릴 | 游릳 | 
|LLAMA| meta-llama/Llama-2-70b-hf | 游릴 | 游릴 | 游릴 | 游릴 | 游릳 | 
|GPT-J| EleutherAI/gpt-j-6b | 游릴 | 游릴 | 游릴 | 游릴 | 游릴 | 
|GPT-NEOX| EleutherAI/gpt-neox-20b | 游릴 | 游릳 | 游릳 | 游릴 | 游릳 | 
|DOLLY| databricks/dolly-v2-12b | 游릴 | 游릳 | 游릳 | 游릴 | 游릳 | 
|FALCON| tiiuae/falcon-40b | 游릴 | 游릴 | 游릴 | 游릴 | 游릴 | 
|OPT| facebook/opt-30b | 游릴 | 游릴 | 游릴 | 游릴 | 游릳 | 
|OPT| facebook/opt-1.3b | 游릴 | 游릴 | 游릴 | 游릴 | 游릳 | 
|Bloom| bigscience/bloom-1b7 | 游릴 | 游릳 | 游릴 | 游릴  | 游릳 |
|CodeGen| Salesforce/codegen-2B-multi | 游릴 | 游릴 | 游릳 | 游릴 | 游릴 |
|Baichuan| baichuan-inc/Baichuan2-7B-Chat | 游릴 | 游릴 | 游릴 | 游릴 |    |
|Baichuan| baichuan-inc/Baichuan2-13B-Chat | 游릴 | 游릴 | 游릴 | 游릴 |    |
|Baichuan| baichuan-inc/Baichuan-13B-Chat | 游릴 | 游릳 | 游릴 | 游릴 |    |
|ChatGLM| THUDM/chatglm3-6b | 游릴 | 游릴 | 游릳 | 游릴 |    |
|ChatGLM| THUDM/chatglm2-6b | 游릴 | 游릴 | 游릳 | 游릴 |    |
|GPTBigCode| bigcode/starcoder | 游릴 | 游릴 | 游릳 | 游릴 | 游릳 |
|T5| google/flan-t5-xl | 游릴 | 游릴 | 游릳 | 游릴 |    |
|Mistral| mistralai/Mistral-7B-v0.1 | 游릴 | 游릴 | 游릳 | 游릴 | 游릳 |
|MPT| mosaicml/mpt-7b | 游릴 | 游릴 | 游릳 | 游릴 | 游릴 |

- 游릴 signifies that the model can perform well and with good accuracy (<1% difference as compared with FP32).

- 游릳 signifies that the model can perform well while accuracy may not been in a perfect state (>1% difference as compared with FP32).

*Note*: The above verified models (including other models in the same model family, like "codellama/CodeLlama-7b-hf" from LLAMA family) are well supported with all optimizations like indirect access KV cache, fused ROPE, and prepacked TPP Linear (fp32/bf16).
We are working in progress to better support the models in the tables with various data types. In addition, more models will be optimized in the future.

## Support

The team tracks bugs and enhancement requests using [GitHub issues](https://github.com/intel/intel-extension-for-pytorch/issues/). Before submitting a suggestion or bug report, search the existing GitHub issues to see if your issue has already been reported.

## Intel춽 AI Reference Models

Use cases that had already been optimized by Intel engineers are available at [Intel춽 AI Reference Models](https://github.com/IntelAI/models/tree/pytorch-r2.2-models) (former Model Zoo). A bunch of PyTorch use cases for benchmarking are also available on the [Github page](https://github.com/IntelAI/models/tree/pytorch-r2.2-models/benchmarks#pytorch-use-cases). You can get performance benefits out-of-box by simply running the scripts in the Reference Models.

## License

_Apache License_, Version _2.0_. As found in [LICENSE](https://github.com/intel/intel-extension-for-pytorch/blob/main/LICENSE) file.

## Security

See Intel's [Security Center](https://www.intel.com/content/www/us/en/security-center/default.html)
for information on how to report a potential security issue or vulnerability.

See also: [Security Policy](SECURITY.md)

