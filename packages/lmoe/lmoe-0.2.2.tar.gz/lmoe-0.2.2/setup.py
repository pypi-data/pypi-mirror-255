# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['lmoe', 'lmoe.api', 'lmoe.experts', 'lmoe.framework', 'lmoe.utils']

package_data = \
{'': ['*'], 'lmoe.experts': ['templates/*']}

install_requires = \
['PySocks>=1.7.1,<2.0.0',
 'injector>=0.21.0,<0.22.0',
 'ollama>=0.1.6,<0.2.0',
 'pyperclip>=1.8.2,<2.0.0',
 'pytype>=2024.1.24,<2025.0.0',
 'requests>=2.31.0,<3.0.0',
 'urllib3==1.26.15']

entry_points = \
{'console_scripts': ['lmoe = lmoe.main:run']}

setup_kwargs = {
    'name': 'lmoe',
    'version': '0.2.2',
    'description': "lmoe (Layered Mixture of Experts,'Elmo') is your programmable CLI assistant.",
    'long_description': '# Introduction\n\n<img src="https://rybosome.github.io/lmoe/assets/lmoe-armadillo.png">\n\n`lmoe` (Layered Mixture of Experts, pronounced "Elmo") is a programmable, multimodal CLI assistant\nwith a natural language interface.\n\nRunning on Ollama and various open-weight models, `lmoe` is a simple, yet powerful way to\ninteract with highly configurable AI models from the command line.\n\n## Setup\n\nYou may wish to install `lmoe` in a virtual environment.\n\n```\n% pip install lmoe\n```\n\nEnsure that an Ollama server is running, then manually initialize the root classification model.\n\n```\n% lmoe --classifier_modelfile > temp-classifier-modelfile.txt\n% ollama create lmoe_classifier -f temp-classifier-modelfile.txt\n```\n\nFinally, refresh the rest of the models.\n\n```\n% lmoe refresh\n```\n\nFurther interaction wtih `lmoe` may cause Ollama to pull any models not present on your local machine.\n\n## Overview\n\n### Natural language querying\n```\n% lmoe who was matisse\n\n Henri Matisse was a French painter, sculptor, and printmaker, known for his influential role in\n modern art. He was born on December 31, 1869, in Le Cateau-Cambrésis, France, and died on\n November 3, 1954. Matisse is recognized for his use of color and his fluid and expressive\n brushstrokes. His works include iconic paintings such as "The Joy of Life" and "Woman with a Hat."\n```\n\n### Piping context\n\n```\n% cat projects/lmoe/lmoe/main.py | lmoe what does this code do\n\n The provided code defines a Python script named \'lmoe\' which includes an argument parser, the\n ability to read context from both STDIN and the clipboard, and a \'classifier\' module for\n determining which expert should respond to a query without actually responding. It does not contain\n any functionality for executing queries or providing responses itself. Instead, it sets up the\n infrastructure for interfacing with external experts through their \'generate\' methods.\n```\n\n```\n% ls -la $HOME | lmoe how big is my zsh history\n\n The size of your Zsh history file is 16084 bytes.\n```\n\n### Pasting context\n\n```\n% print -x \'hello\'\nprint: positive integer expected after -x: hello\n```\n\nCopy this to the clipboard, then:\n\n```\n% lmoe --paste how do I fix this error\n To use the `-x` option with the `print` command in Bash, you need to provide a positional argument that is a file descriptor. Instead, you provided a string \'hello\'. Here\'s how you can correctly use it:\n\n1. Create or have a file with the name \'hello\' and make sure it exists in your working directory.\n2. Run the following command instead: `print -r -- < hello`. This reads the contents of the file \'hello\' as input for print, which displays its output to stdout.\n```\n\n### Sequencing\n\n`lmoe` can be piped into itself. This allows scriptable composition of primitives into more advanced\nfunctionality.\n\n```\n% lmoe what is the recommended layout for a python project with poetry |\nlmoe "make a project like this for a module called \'alexandria\' with 3 sub modules: \'auth\', \'util\', and \'io\'"\n\n mkdir alexandria/\n touch alexandria/pyproject.toml\n touch alexandria/README.rst\n touch alexandria/requirements.in\n mkdir alexandria/src/\n touch alexandria/src/__init__.py\n mkdir alexandria/src/alexandria/\n touch alexandria/src/alexandria/__init__.py\n mkdir alexandria/src/alexandria/auth/\n touch alexandria/src/alexandria/auth/__init__.py\n touch alexandria/src/alexandria/util/\n touch alexandria/src/alexandria/util/__init__.py\n touch alexandria/src/alexandria/io/\n touch alexandria/src/alexandria/io/__init__.py\n```\n\n## Capabilities\n\n`lmoe` supports a number of specific functions beyond general LLM querying and instruction.\n\nMore coming soon.\n\n### Image Recognition\n\n**Note**: currently this is raw, unparsed JSON output. Edited by hand for clarity in reading.\n\nThis is `lmoe`\'s first attempt to describe its default avatar.\n\n```\n% curl -sS \'https://rybosome.github.io/lmoe/assets/lmoe-armadillo.png\' | base64 -i - | lmoe what is in this picture\n{\n    "model":"llava",\n    "created_at":"2024-02-08T07:09:28.827507Z",\n    "response":" The image features a stylized, colorful creature that appears to be a combination\n                 of different animals. It has the body of a rat, with a prominent tail and ears,\n                 which is also typical of rats. The head resembles a cat, with pointy ears and what\n                 seems to be cat whiskers. The creature has eyes like those of a cat, and it\'s\n                 wearing a helmet or headgear that looks like an advanced robot with digital\n                 readouts on the forehead, giving it a cyberpunk aesthetic. The background is\n                 colorful with a rainbow pattern, enhancing the fantastical nature of the creature.\n                 This image is likely a piece of digital art designed to showcase imaginative and\n                 creative concepts. ",\n    "done":true,\n    "context":[733,16289,28793,767,349,297,456,5754,733,28748,16289,28793,415,3469,4190,264,341,2951,1332,28725,3181,1007,15287,369,8045,298,347,264,9470,302,1581,8222,28723,661,659,272,2187,302,264,6172,28725,395,264,15574,8675,304,12446,28725,690,349,835,10842,302,408,1449,28723,415,1335,312,5650,867,264,5255,28725,395,1305,28724,12446,304,767,3969,298,347,5255,26898,404,28723,415,15287,659,2282,737,1395,302,264,5255,28725,304,378,28742,28713,8192,264,26371,442,1335,490,283,369,4674,737,396,10023,18401,395,7153,1220,8508,356,272,18522,28725,5239,378,264,23449,28720,2060,27974,28723,415,5414,349,3181,1007,395,264,7296,11809,5340,28725,8050,7161,272,7399,529,745,4735,302,272,15287,28723,851,3469,349,3917,264,5511,302,7153,1524,5682,298,1347,2210,26671,1197,304,9811,16582,28723,28705],"total_duration":7148311208,"load_duration":2687336958,\n    "prompt_eval_count":1,\n    "prompt_eval_duration":1313448000,\n    "eval_count":151,\n    "eval_duration":3111945000}\n```\n\n### Project Generation\n\n*Generate a new programming project from an ascii or textual description.*\n\n```\n% lmoe what is the recommended layout for a python project with poetry\n With Poetry, a Python packaging and project management tool, a recommended layout for a Python\n project could include the following structure:\n\n myproject/\n ├── pyproject.toml\n ├── README.rst\n ├── requirements.in\n └── src/\n     ├── __init__.py\n     └── mypackage/\n         ├── __init__.py\n         ├── module1.py\n         └── module2.py\n\nIn this layout, the `myproject/` directory contains the root-level project files. The\n`pyproject.toml` file is used for managing dependencies and building your Python package. The\n`README.rst` file is optional, but common, to include documentation about your project. The\n`requirements.in` file lists the external packages required by your project.\n\nThe `src/` directory contains your source code for the project. In this example, there\'s a package\nnamed `mypackage`, which includes an `__init__.py` file and two modules: `module1.py` and\n`module2.py`.\n\nThis is just one suggested layout using Poetry. Depending on your specific project requirements and\npreferences, the layout might vary. Always refer to the [Poetry documentation](https://python-poetry.org/)\nfor more detailed information.\n```\n\nCopy this to the clipboard, and then:\n\n```\n% lmoe --paste "make a project like this for a module called \'alexandria\' with 3 sub modules: \'auth\', \'util\', and \'io\'"\nmkdir alexandria/\ntouch alexandria/pyproject.toml\ntouch alexandria/README.rst\ntouch alexandria/requirements.in\nmkdir alexandria/src/\ntouch alexandria/src/__init__.py\nmkdir alexandria/src/alexandria/\ntouch alexandria/src/alexandria/__init__.py\nmkdir alexandria/src/alexandria/auth/\ntouch alexandria/src/alexandria/auth/__init__.py\nmkdir alexandria/src/alexandria/util/\ntouch alexandria/src/alexandria/util/__init__.py\nmkdir alexandria/src/alexandria/io/\ntouch alexandria/src/alexandria/io/__init__.py\n```\n\n...for a list of runnable shell commands.\n\nComing soon: `lmoe` will offer to run them for you, open them in an editor, or stop.\n\n### Utilities\n\nCapabilities with multiple inputs listed are examples of different ways to activate it.\n\n#### Refresh\n\n*Update local Ollama modelfiles.*\n\nThis should be run any time you add a new expert, modelfile, or\nalter a modelfile template.\n\n```\n% lmoe refresh\n% lmoe update your models\n% lmoe refresh the models\n% lmoe update models\n\nDeleting existing lmoe_classifier...\nUpdating lmoe_classifier...\nDeleting existing lmoe_code...\nUpdating lmoe_code...\nDeleting existing lmoe_project_initialization...\nUpdating lmoe_project_initialization...\nDeleting existing lmoe_general...\nUpdating lmoe_general...\n```\n\n#### Model Listing\n\n*List Ollama metadata on models used internally by `lmoe`.*\n\n```\n% lmoe list\n% lmoe what are your models\n% lmoe list your models\n\n{\'name\': \'lmoe_classifier:latest\', \'model\': \'lmoe_classifier:latest\', \'modified_at\': \'2024-02-05T13:46:49.983916538-08:00\', \'size\': 4109868691, \'digest\': \'576c04e5f9c9e82b2ca14cfd5754ca56610619cddb737a6ca968d064c86bcb68\', \'details\': {\'parent_model\': \'\', \'format\': \'gguf\', \'family\': \'llama\', \'families\': [\'llama\'], \'parameter_size\': \'7B\', \'quantization_level\': \'Q4_0\'}}\n{\'name\': \'lmoe_code:latest\', \'model\': \'lmoe_code:latest\', \'modified_at\': \'2024-02-05T13:46:49.988112317-08:00\', \'size\': 4109866128, \'digest\': \'f387ef329bc0ebd9df25dcc8c4f014bbbe127e6a543c8dfa992a805d71fbbb1e\', \'details\': {\'parent_model\': \'\', \'format\': \'gguf\', \'family\': \'llama\', \'families\': [\'llama\'], \'parameter_size\': \'7B\', \'quantization_level\': \'Q4_0\'}}\n{\'name\': \'lmoe_general:latest\', \'model\': \'lmoe_general:latest\', \'modified_at\': \'2024-02-05T13:46:49.996594585-08:00\', \'size\': 4109867476, \'digest\': \'657788601d06890ac136d61bdecec9e3a8ebff4e9139c5cc0fbfa56377625d25\', \'details\': {\'parent_model\': \'\', \'format\': \'gguf\', \'family\': \'llama\', \'families\': [\'llama\'], \'parameter_size\': \'7B\', \'quantization_level\': \'Q4_0\'}}\n{\'name\': \'lmoe_project_initialization:latest\', \'model\': \'lmoe_project_initialization:latest\', \'modified_at\': \'2024-02-05T13:46:49.991328433-08:00\', \'size\': 4109868075, \'digest\': \'9af2d395e8883910952bee2668d18131206fb5c612bc5d4a207b6637e1bc6907\', \'details\': {\'parent_model\': \'\', \'format\': \'gguf\', \'family\': \'llama\', \'families\': [\'llama\'], \'parameter_size\': \'7B\', \'quantization_level\': \'Q4_0\'}}\n```\n\n## Extension Model\n\nNew capabilities can be added to `lmoe` with low overhead. All capabilities, internal and\nuser-defined, are implemented with the same programming model.\n\nJust implement [lmoe.api.base_expert.BaseExpert](https://github.com/rybosome/lmoe/blob/main/lmoe/api/base_expert.py) and add your new expert to the registry in\n`lmoe/experts/__init__.py`. See existing experts for examples.\n\nMore to come as API finalizes - moving to dependency injection in the next update.\n\n## Status\n\nVersion 0.2.2\n\nThis is currently a very basic implementation.\n\nSupports a general expert and image recognition.\n\nNot configurable, limited automation for environment setup, and does not have persistence.\n\nThis is not yet ready for others\' use.\n\n### Upcoming features\n\n* dependency injection\n* error handling\n* self-setup of models and ollama context after installation\n* persisted context (i.e. memory, chat-like experience without a formal chat interface)\n* configurability\n* tests\n* further tuning of classification, code generation, and project initialization\n* dry-run for mutating actions, ability to execute mutating actions\n* many more commands\n  * filesystem interaction\n    * finding file contents from various queries (specific file path, fuzzy description, "this directory", etc.)\n  * executors for existing bash commands\n    * awk\n    * curl\n  * API clients\n    * weather\n    * wikipedia\n\n## Lmoe Armadillo\n\nThe avatar of `lmoe` is Lmoe Armadillo, a cybernetic [Cingulata](https://en.wikipedia.org/wiki/Cingulata)\nwho is ready to dig soil and execute toil.\n\nLmoe Armadillo is a curious critter who assumes many different manifestations.\n\n![Lmoe\'s default avatar against a lit background](https://rybosome.github.io/lmoe/assets/lmoe-armadillo-alt4-380px.jpg)\n![An alternative Lmoe with a cute face](https://rybosome.github.io/lmoe/assets/lmoe-armadillo-alt1-380px.jpg)\n![A blue-nosed Lmoe Armadillo](https://rybosome.github.io/lmoe/assets/lmoe-armadillo-alt3-380px.jpg)\n![A realistic Lmoe Armadillo against a surrealist backdrop](https://rybosome.github.io/lmoe/assets/lmoe-armadillo-alt2-380px.jpg)\n',
    'author': 'Ryan Eiger',
    'author_email': 'ryebosome@gmail.com',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'None',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'entry_points': entry_points,
    'python_requires': '>=3.9,<4.0',
}


setup(**setup_kwargs)
